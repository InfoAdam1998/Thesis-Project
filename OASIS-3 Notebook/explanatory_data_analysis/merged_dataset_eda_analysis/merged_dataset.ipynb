{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now better understand the spread of our dataset and detect a few outliers from here. \n",
    "\n",
    "How about we check the Q-Q plot for each variable? Perhaps we can have a much better comprehension if the data follows a normal distribution aside from relying on the above histogram.\n",
    "\n",
    "Note: *Q-Q plots rely on the complete dataset to compute quantiles. Therefore, i had to drop a few values to see the red line*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Q-Q plots for each column\n",
    "plt.figure(figsize=(20, 45))\n",
    "\n",
    "for i in range(len(dataset.columns)):\n",
    "    plt.subplot(10, 4, i + 1)\n",
    "    stats.probplot(dataset[dataset.columns[i]].dropna(), \n",
    "                   dist='norm', plot=plt)\n",
    "    plt.title(dataset.columns[i])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Interesing, perhaps the distribution may change if we perform data imputation? If thats the case, then we may need to perform data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "results = {}\n",
    "\n",
    "for column in dataset.columns:  \n",
    "    stat, p = shapiro(dataset[column].dropna())\n",
    "    skewness = skew(dataset[column].dropna())\n",
    "    kurt = kurtosis(dataset[column].dropna())\n",
    "    results[column] = {'p-value': p, 'Normal': p > 0.05, 'Skewness': skewness, 'Kurtosis': kurt} \n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each column using KDE\n",
    "plt.figure(figsize=(20, 45))\n",
    "\n",
    "for i in range(len(dataset.columns)):\n",
    "    plt.subplot(12,3,i+1)\n",
    "    sns.kdeplot(data = dataset, \n",
    "                 x = dataset[dataset.columns[i]], \n",
    "                 hue = \"DIAGNOSIS\",\n",
    "                 multiple=\"stack\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots\n",
    "plt.figure(figsize=(20, 45))\n",
    "\n",
    "for i in range(len(dataset.columns)):\n",
    "    plt.subplot(10,4,i+1)\n",
    "    sns.boxplot(data = dataset,\n",
    "                y = dataset[dataset.columns[i]],\n",
    "                x = \"DIAGNOSIS\", \n",
    "                color=\"#5e76fe\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation dataframe\n",
    "dataset_correlation = dataset.corr()\n",
    "dataset_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap\n",
    "plt.figure(figsize=(20, 20))  \n",
    "\n",
    "sns.heatmap(dataset_correlation, \n",
    "            annot = True, \n",
    "            fmt = \".3f\", \n",
    "            cmap = \"coolwarm\",\n",
    "            square = True,\n",
    "            linewidths = 0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_pairs = pg.pairwise_corr(data = dataset)\n",
    "\n",
    "correlation_report = correlation_pairs[correlation_pairs[\"r\"].abs() > 0.7]\n",
    "\n",
    "correlation_report[[\"X\", \"Y\", \"r\"]].sort_values(\"r\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset.drop(\"DIAGNOSIS\", axis=1),  \n",
    "    dataset[\"DIAGNOSIS\"],  \n",
    "    test_size=0.3,  \n",
    "    random_state=42,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_bayes = IterativeImputer(\n",
    "    estimator=BayesianRidge(),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "imputer_knn = IterativeImputer(\n",
    "    estimator=KNeighborsRegressor(),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "imputer_nonLin = IterativeImputer(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "imputer_missForest = IterativeImputer(\n",
    "    estimator=ExtraTreesRegressor(),\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_bayes.fit(X_train)\n",
    "imputer_knn.fit(X_train)\n",
    "\n",
    "imputer_nonLin.fit(X_train)\n",
    "imputer_missForest.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NA\n",
    "X_train_bayes = imputer_bayes.transform(X_train)\n",
    "X_test_bayes = imputer_bayes.transform(X_test)\n",
    "\n",
    "X_train_knn = imputer_knn.transform(X_train)\n",
    "X_test_knn = imputer_knn.transform(X_test)\n",
    "\n",
    "X_train_nonLin = imputer_nonLin.transform(X_train)\n",
    "X_test_nonLin = imputer_nonLin.transform(X_test)\n",
    "\n",
    "X_train_missForest = imputer_missForest.transform(X_train)\n",
    "X_test_missForest = imputer_missForest.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bayes_t = pd.DataFrame(X_train_bayes, columns=X_train.columns)\n",
    "X_test_bayes_t = pd.DataFrame(X_test_bayes, columns=X_test.columns)\n",
    "\n",
    "X_train_knn_t = pd.DataFrame(X_train_knn, columns=X_train.columns)\n",
    "X_test_knn_t = pd.DataFrame(X_test_knn, columns=X_test.columns)\n",
    "\n",
    "X_train_nonLin_t = pd.DataFrame(X_train_nonLin, columns=X_train.columns)\n",
    "X_test_nonLin_t = pd.DataFrame(X_test_nonLin, columns=X_test.columns)\n",
    "\n",
    "X_train_missForest_t = pd.DataFrame(X_train_missForest, columns=X_train.columns)\n",
    "X_test_missForest_t = pd.DataFrame(X_test_missForest, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [\n",
    "    X_train_bayes_t, X_test_bayes_t, \n",
    "    X_train_knn_t, X_test_knn_t,\n",
    "    X_train_nonLin_t, X_test_nonLin_t,\n",
    "    X_train_missForest_t, X_test_missForest_t,\n",
    "]\n",
    "\n",
    "for df in dataframes:\n",
    "    if df.isna().any().all() == False:\n",
    "        print(\"No missing values in any dataframe\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Still missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, balanced_accuracy_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "def run_random_forest(classifier_name, imp_method, X_train, y_train):\n",
    "\n",
    "        # Initialize the Random Forest with class weight balancing\n",
    "        rf = RandomForestClassifier(n_estimators=100,\n",
    "                                    max_depth=5,\n",
    "                                    random_state=42,\n",
    "                                    class_weight=\"balanced\",\n",
    "        )\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=5,\n",
    "                             shuffle=True,\n",
    "                             random_state=42,\n",
    "        )\n",
    "        \n",
    "        metrics = {\"accuracy\": make_scorer(accuracy_score),\n",
    "                   \"balanced_accuracy\": make_scorer(balanced_accuracy_score),  \n",
    "                   \"precision\": make_scorer(precision_score, average=\"weighted\"), \n",
    "                   \"recall\": make_scorer(recall_score, average=\"weighted\"), \n",
    "                   \"f1_weighted\": make_scorer(f1_score, average=\"weighted\"),\n",
    "                   \"roc_auc_ovr_weighted\": make_scorer(roc_auc_score, \n",
    "                                                       average=\"weighted\", \n",
    "                                                       multi_class=\"ovr\", \n",
    "                                                       response_method=\"predict_proba\",),\n",
    "        }\n",
    "        \n",
    "        cross_val_results = cross_validate(rf,\n",
    "                                           X_train,\n",
    "                                           y_train,\n",
    "                                           cv=kf,\n",
    "                                           scoring=metrics,\n",
    "                                           return_train_score=True,\n",
    "        )\n",
    "                \n",
    "        metric_names = list(metrics.keys())\n",
    "        mean_train = [round(np.mean(cross_val_results[f\"train_{metric}\"]), 3) for metric in metric_names]\n",
    "        std_train = [round(np.std(cross_val_results[f\"train_{metric}\"]), 3) for metric in metric_names]\n",
    "        mean_test = [round(np.mean(cross_val_results[f\"test_{metric}\"]), 3) for metric in metric_names]\n",
    "        std_test = [round(np.std(cross_val_results[f\"test_{metric}\"]), 3) for metric in metric_names]\n",
    "        time = round(np.mean(cross_val_results[f\"fit_time\"]), 3)\n",
    "                \n",
    "        cv_metrics_df = pd.DataFrame({\n",
    "                \"Classifier\": classifier_name,\n",
    "                \"Imputation\": imp_method,\n",
    "                \"Fit Time\": time,\n",
    "                \"Metric\": metric_names,\n",
    "                \"Mean Train\": mean_train,\n",
    "                \"Std Train\": std_train,\n",
    "                \"Mean Test\": mean_test,\n",
    "                \"Std Test\": std_test,\n",
    "        })\n",
    "        \n",
    "        fit_model = rf.fit(X_train, y_train)\n",
    "        \n",
    "        return fit_model, cv_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model, bayes_metrics = run_random_forest(\"Random Forest\", \"Ridge Bayes\", X_train_bayes_t, y_train)\n",
    "knn_model, knn_metrics = run_random_forest(\"Random Forest\", \"KNN Reg\", X_train_knn_t, y_train)\n",
    "nonLin_model, nonLin_metrics = run_random_forest(\"Random Forest\", \"NonLin\", X_train_nonLin_t, y_train)\n",
    "missForest_model, missForest_metrics = run_random_forest(\"Random Forest\", \"MissForest\", X_train_missForest_t, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.concat([bayes_metrics, knn_metrics, nonLin_metrics, missForest_metrics])\n",
    "validation_df_report = validation_df.set_index([\"Classifier\", \"Imputation\", \"Fit Time\", \"Metric\"])\n",
    "validation_df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def eval_random_forest(classifier_name, imp_method, model, X_train, X_test, y_train, y_test):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # Get predicted probabilities for ROC AUC\n",
    "        pred_train_proba = model.predict_proba(X_train)\n",
    "        pred_test_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # Get predicted accuracy values\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics_train = {\n",
    "                \"accuracy\": round(accuracy_score(y_train, pred_train), 3),\n",
    "                \"balanced_accuracy\": round(balanced_accuracy_score(y_train, pred_train), 3),\n",
    "                \"precision\": round(precision_score(y_train, pred_train, average=\"weighted\"), 3),\n",
    "                \"recall\": round(recall_score(y_train, pred_train, average=\"weighted\"), 3),\n",
    "                \"f1_weighted\": round(f1_score(y_train, pred_train, average=\"weighted\"), 3),\n",
    "                \"roc_auc_ovr_weighted\": round(roc_auc_score(y_train, pred_train_proba, average=\"weighted\", multi_class=\"ovr\"),3,),\n",
    "        }\n",
    "        \n",
    "        metrics_test = {\n",
    "                \"accuracy\": round(accuracy_score(y_test, pred_test), 3),\n",
    "                \"balanced_accuracy\": round(balanced_accuracy_score(y_test, pred_test), 3),\n",
    "                \"precision\": round(precision_score(y_test, pred_test, average=\"weighted\"), 3),\n",
    "                \"recall\": round(recall_score(y_test, pred_test, average=\"weighted\"), 3),\n",
    "                \"f1_weighted\": round(f1_score(y_test, pred_test, average=\"weighted\"), 3),\n",
    "                \"roc_auc_ovr_weighted\": round(roc_auc_score(y_test, pred_test_proba, average=\"weighted\", multi_class=\"ovr\"),3,),\n",
    "        }\n",
    "                \n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "        # Create the DataFrame without additional rounding\n",
    "        pred_metrics_df = pd.DataFrame({\n",
    "                \"Classifier\": classifier_name,\n",
    "                \"Imputation\": imp_method,\n",
    "                \"Classification Time\": round(elapsed_time, 3),\n",
    "                \"Metric\": metrics_train.keys(),\n",
    "                \"Train data\": metrics_train.values(),\n",
    "                \"Test data\": metrics_test.values(),\n",
    "    })\n",
    "        \n",
    "        return pred_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bayes = eval_random_forest(\"Random Forest\", \"Ridge Bayes\", bayes_model, X_train_bayes_t, X_test_bayes_t, y_train, y_test)\n",
    "pred_knn = eval_random_forest(\"Random Forest\", \"KNN Reg\", knn_model, X_train_knn_t, X_test_knn_t, y_train, y_test)\n",
    "pred_nonLin = eval_random_forest(\"Random Forest\", \"NonLin\", nonLin_model, X_train_nonLin_t, X_test_nonLin_t, y_train, y_test)\n",
    "pred_missForest = eval_random_forest(\"Random Forest\", \"MissForest\", missForest_model, X_train_missForest_t, X_test_missForest_t, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.concat([pred_bayes, pred_knn, pred_nonLin, pred_missForest])\n",
    "prediction_df_report = prediction_df.set_index([\"Classifier\", \"Imputation\",\t\"Classification Time\", \"Metric\"])\n",
    "prediction_df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.selection import DropConstantFeatures, DropDuplicateFeatures, RecursiveFeatureElimination\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from ngboost import NGBClassifier\n",
    "from ngboost.distns import k_categorical\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def run_model(classifier_name, X_train, y_train):\n",
    "        \n",
    "        columns = len(X_train.columns)\n",
    "        \n",
    "        # Initialize the model based on classifier_name\n",
    "        if classifier_name == \"Random Forest\":\n",
    "                model = RandomForestClassifier(n_estimators=100, \n",
    "                                        max_depth=5, \n",
    "                                        random_state=42, \n",
    "                                        class_weight=\"balanced\")\n",
    "                \n",
    "        elif classifier_name == \"XGBoost\":\n",
    "        # XGBClassifier sometimes issues warnings about label encoding. \n",
    "        # We disable the use_label_encoder and set an evaluation metric.\n",
    "                model = XGBClassifier(n_estimators=100, \n",
    "                                max_depth=5, \n",
    "                                random_state=42,\n",
    "                                eval_metric=\"logloss\")\n",
    "        \n",
    "        elif classifier_name == \"NGBoost\":\n",
    "                model = NGBClassifier(Dist=k_categorical(len(np.unique(y_train))),\n",
    "                                      n_estimators=100,\n",
    "                                      verbose=False)\n",
    "                \n",
    "        elif classifier_name == \"Logistic Regression\":\n",
    "                model = LogisticRegression(\n",
    "                class_weight=\"balanced\",\n",
    "                solver=\"lbfgs\",\n",
    "                max_iter=500,\n",
    "                random_state=42\n",
    "                )\n",
    "                \n",
    "                # Scale features\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "        else:\n",
    "                raise ValueError(\"Unknown classifier\")\n",
    "                \n",
    "        kf = StratifiedKFold(n_splits=5,\n",
    "                             shuffle=True,\n",
    "                             random_state=42,\n",
    "        )\n",
    "        \n",
    "        cross_val_results = cross_validate(model,\n",
    "                                           X_train,\n",
    "                                           y_train,\n",
    "                                           cv=kf,\n",
    "                                           scoring=\"balanced_accuracy\",\n",
    "                                           return_train_score=True,\n",
    "        )\n",
    "               \n",
    "        # Calculate the metrics for training and testing\n",
    "        mean_train = [round(np.mean(cross_val_results[\"train_score\"]), 3)]\n",
    "        std_train = [round(np.std(cross_val_results[\"train_score\"]), 3)]\n",
    "        mean_test = [round(np.mean(cross_val_results[\"test_score\"]), 3)]\n",
    "        std_test = [round(np.std(cross_val_results[\"test_score\"]), 3)]\n",
    "        time = round(np.mean(cross_val_results[\"fit_time\"]), 3)\n",
    "                \n",
    "        cv_metrics_df = pd.DataFrame({\n",
    "                \"Classifier\": classifier_name,\n",
    "                \"Fit Time\": time,\n",
    "                \"Total Features\": columns,\n",
    "                \"Metric\": \"balanced_accuracy\",\n",
    "                \"Mean Train\": mean_train,\n",
    "                \"Std Train\": std_train,\n",
    "                \"Mean Test\": mean_test,\n",
    "                \"Std Test\": std_test,\n",
    "        })\n",
    "        \n",
    "        fit_model = model.fit(X_train, y_train)\n",
    "        \n",
    "        return fit_model, cv_metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rf_n, metrics_rf_n = run_model(\"Random Forest\", X_train_knn_t, y_train)\n",
    "\n",
    "trained_xgb_n, metrics_xgb_n = run_model(\"XGBoost\", X_train_knn_t, y_train)\n",
    "\n",
    "trained_ngb_n, metrics_ngb_n = run_model(\"NGBoost\", X_train_knn_t, y_train)\n",
    "\n",
    "trained_lg_n, metrics_lg_n = run_model(\"Logistic Regression\", X_train_knn_t, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.concat([metrics_rf_n, metrics_xgb_n, metrics_ngb_n, metrics_lg_n])\n",
    "validation_df_report = validation_df.set_index([\"Classifier\", \"Metric\", \"Fit Time\", \"Total Features\"])\n",
    "validation_df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def eval_model(classifier_name, model, X_train, X_test, y_train, y_test):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        columns = len(X_train.columns)\n",
    "        \n",
    "        if classifier_name == \"Logistic Regression\":  \n",
    "                # Scale features\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "        # Get predicted probabilities for ROC AUC\n",
    "        pred_train_proba = model.predict_proba(X_train)\n",
    "        pred_test_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # Get predicted accuracy values\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics_train = {\n",
    "                \"accuracy\": round(accuracy_score(y_train, pred_train), 3),\n",
    "                \"balanced_accuracy\": round(balanced_accuracy_score(y_train, pred_train), 3),\n",
    "                \"precision\": round(precision_score(y_train, pred_train, average=\"weighted\"), 3),\n",
    "                \"recall\": round(recall_score(y_train, pred_train, average=\"weighted\"), 3),\n",
    "                \"f1_weighted\": round(f1_score(y_train, pred_train, average=\"weighted\"), 3),\n",
    "                \"roc_auc_ovr_weighted\": round(roc_auc_score(y_train, pred_train_proba, average=\"weighted\", multi_class=\"ovr\"),3,),\n",
    "        }\n",
    "        \n",
    "        metrics_test = {\n",
    "                \"accuracy\": round(accuracy_score(y_test, pred_test), 3),\n",
    "                \"balanced_accuracy\": round(balanced_accuracy_score(y_test, pred_test), 3),\n",
    "                \"precision\": round(precision_score(y_test, pred_test, average=\"weighted\"), 3),\n",
    "                \"recall\": round(recall_score(y_test, pred_test, average=\"weighted\"), 3),\n",
    "                \"f1_weighted\": round(f1_score(y_test, pred_test, average=\"weighted\"), 3),\n",
    "                \"roc_auc_ovr_weighted\": round(roc_auc_score(y_test, pred_test_proba, average=\"weighted\", multi_class=\"ovr\"),3,),\n",
    "        }\n",
    "                \n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "        # Create the DataFrame without additional rounding\n",
    "        pred_metrics_df = pd.DataFrame({\n",
    "                \"Classifier\": classifier_name,\n",
    "                \"Classification Time\": round(elapsed_time, 3),\n",
    "                \"Total Features\": columns,\n",
    "                \"Metric\": metrics_train.keys(),\n",
    "                \"Train data\": metrics_train.values(),\n",
    "                \"Test data\": metrics_test.values(),\n",
    "    })\n",
    "        \n",
    "        return pred_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"all\"\n",
    "prediction_df_report = None\n",
    "\n",
    "if results == \"all\":\n",
    "    pred_rf_n = eval_model(\"Random Forest\", trained_rf_n, X_train_knn_t, X_test_knn_t, y_train, y_test)\n",
    "\n",
    "    pred_xgb_n = eval_model(\"XGBoost\", trained_xgb_n, X_train_knn_t, X_test_knn_t, y_train, y_test)\n",
    "\n",
    "    pred_ngb_n = eval_model(\"NGBoost\", trained_ngb_n, X_train_knn_t, X_test_knn_t, y_train, y_test)\n",
    "    \n",
    "    pred_lg_n = eval_model(\"Logistic Regression\", trained_lg_n, X_train_knn_t, X_test_knn_t, y_train, y_test)\n",
    "    \n",
    "    prediction_df = pd.concat([pred_rf_n, pred_xgb_n, pred_ngb_n, pred_lg_n])\n",
    "    prediction_df_report = prediction_df.set_index([\"Classifier\", \"Classification Time\", \"Total Features\", \"Metric\"])\n",
    "\n",
    "prediction_df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
