{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSE0m</th>\n",
       "      <th>HipsASMbaseline</th>\n",
       "      <th>HipsContrastbaseline</th>\n",
       "      <th>HipsCorelationbaseline</th>\n",
       "      <th>HipsSumVariancebaseline</th>\n",
       "      <th>HipsEntropybaseline</th>\n",
       "      <th>HipsClusterShadebaseline</th>\n",
       "      <th>ERCsContrastbaseline</th>\n",
       "      <th>ERCsCorelationbaseline</th>\n",
       "      <th>ERCsVariancebaseline</th>\n",
       "      <th>ERCsSumAveragebaseline</th>\n",
       "      <th>ERCsSumVariancebaseline</th>\n",
       "      <th>ERCsClusterShadebaseline</th>\n",
       "      <th>ERCs_thicknessbaseline</th>\n",
       "      <th>ERCsVolumebaseline</th>\n",
       "      <th>HipposcampusVolumebaseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>170.02</td>\n",
       "      <td>0.45</td>\n",
       "      <td>615.39</td>\n",
       "      <td>3.60</td>\n",
       "      <td>16693.64</td>\n",
       "      <td>257.59</td>\n",
       "      <td>0.43</td>\n",
       "      <td>224.58</td>\n",
       "      <td>29.39</td>\n",
       "      <td>640.73</td>\n",
       "      <td>57.73</td>\n",
       "      <td>2.53</td>\n",
       "      <td>1278.0</td>\n",
       "      <td>2448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>165.75</td>\n",
       "      <td>0.51</td>\n",
       "      <td>550.26</td>\n",
       "      <td>3.59</td>\n",
       "      <td>22784.56</td>\n",
       "      <td>217.25</td>\n",
       "      <td>0.51</td>\n",
       "      <td>217.43</td>\n",
       "      <td>28.37</td>\n",
       "      <td>652.46</td>\n",
       "      <td>2072.42</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>2349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>147.66</td>\n",
       "      <td>0.57</td>\n",
       "      <td>575.46</td>\n",
       "      <td>3.49</td>\n",
       "      <td>7233.57</td>\n",
       "      <td>287.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>233.07</td>\n",
       "      <td>29.95</td>\n",
       "      <td>644.68</td>\n",
       "      <td>-467.36</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>3631.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>184.28</td>\n",
       "      <td>0.58</td>\n",
       "      <td>677.96</td>\n",
       "      <td>3.55</td>\n",
       "      <td>10587.17</td>\n",
       "      <td>204.92</td>\n",
       "      <td>0.52</td>\n",
       "      <td>213.05</td>\n",
       "      <td>29.07</td>\n",
       "      <td>654.06</td>\n",
       "      <td>1421.62</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>3400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>217.01</td>\n",
       "      <td>0.53</td>\n",
       "      <td>696.61</td>\n",
       "      <td>2.83</td>\n",
       "      <td>-507.05</td>\n",
       "      <td>217.01</td>\n",
       "      <td>0.53</td>\n",
       "      <td>228.40</td>\n",
       "      <td>29.23</td>\n",
       "      <td>696.61</td>\n",
       "      <td>-507.05</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>4210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>182.35</td>\n",
       "      <td>0.57</td>\n",
       "      <td>666.08</td>\n",
       "      <td>3.75</td>\n",
       "      <td>10796.82</td>\n",
       "      <td>209.84</td>\n",
       "      <td>0.49</td>\n",
       "      <td>209.96</td>\n",
       "      <td>25.64</td>\n",
       "      <td>630.00</td>\n",
       "      <td>-2180.39</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>2999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>166.95</td>\n",
       "      <td>0.49</td>\n",
       "      <td>516.98</td>\n",
       "      <td>3.83</td>\n",
       "      <td>25330.58</td>\n",
       "      <td>235.69</td>\n",
       "      <td>0.47</td>\n",
       "      <td>222.23</td>\n",
       "      <td>29.77</td>\n",
       "      <td>653.24</td>\n",
       "      <td>1565.56</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1446.0</td>\n",
       "      <td>3163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>178.95</td>\n",
       "      <td>0.58</td>\n",
       "      <td>713.98</td>\n",
       "      <td>3.51</td>\n",
       "      <td>11179.20</td>\n",
       "      <td>204.70</td>\n",
       "      <td>0.53</td>\n",
       "      <td>214.28</td>\n",
       "      <td>24.44</td>\n",
       "      <td>652.40</td>\n",
       "      <td>-3077.22</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1327.0</td>\n",
       "      <td>2918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>168.89</td>\n",
       "      <td>0.52</td>\n",
       "      <td>563.85</td>\n",
       "      <td>3.72</td>\n",
       "      <td>21153.11</td>\n",
       "      <td>207.24</td>\n",
       "      <td>0.51</td>\n",
       "      <td>216.75</td>\n",
       "      <td>29.69</td>\n",
       "      <td>659.77</td>\n",
       "      <td>1771.64</td>\n",
       "      <td>2.81</td>\n",
       "      <td>1801.0</td>\n",
       "      <td>3543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>162.62</td>\n",
       "      <td>0.57</td>\n",
       "      <td>601.44</td>\n",
       "      <td>3.77</td>\n",
       "      <td>16246.81</td>\n",
       "      <td>230.99</td>\n",
       "      <td>0.51</td>\n",
       "      <td>233.95</td>\n",
       "      <td>32.46</td>\n",
       "      <td>704.80</td>\n",
       "      <td>2099.77</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2063.0</td>\n",
       "      <td>3681.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MMSE0m  HipsASMbaseline  HipsContrastbaseline  HipsCorelationbaseline  \\\n",
       "0      24.0             0.09                170.02                    0.45   \n",
       "1      26.0             0.08                165.75                    0.51   \n",
       "2      20.0             0.15                147.66                    0.57   \n",
       "3      29.0             0.12                184.28                    0.58   \n",
       "4      28.0             0.14                217.01                    0.53   \n",
       "..      ...              ...                   ...                     ...   \n",
       "420    30.0             0.11                182.35                    0.57   \n",
       "421    30.0             0.07                166.95                    0.49   \n",
       "422    24.0             0.13                178.95                    0.58   \n",
       "423    27.0             0.08                168.89                    0.52   \n",
       "424    30.0             0.10                162.62                    0.57   \n",
       "\n",
       "     HipsSumVariancebaseline  HipsEntropybaseline  HipsClusterShadebaseline  \\\n",
       "0                     615.39                 3.60                  16693.64   \n",
       "1                     550.26                 3.59                  22784.56   \n",
       "2                     575.46                 3.49                   7233.57   \n",
       "3                     677.96                 3.55                  10587.17   \n",
       "4                     696.61                 2.83                   -507.05   \n",
       "..                       ...                  ...                       ...   \n",
       "420                   666.08                 3.75                  10796.82   \n",
       "421                   516.98                 3.83                  25330.58   \n",
       "422                   713.98                 3.51                  11179.20   \n",
       "423                   563.85                 3.72                  21153.11   \n",
       "424                   601.44                 3.77                  16246.81   \n",
       "\n",
       "     ERCsContrastbaseline  ERCsCorelationbaseline  ERCsVariancebaseline  \\\n",
       "0                  257.59                    0.43                224.58   \n",
       "1                  217.25                    0.51                217.43   \n",
       "2                  287.61                    0.39                233.07   \n",
       "3                  204.92                    0.52                213.05   \n",
       "4                  217.01                    0.53                228.40   \n",
       "..                    ...                     ...                   ...   \n",
       "420                209.84                    0.49                209.96   \n",
       "421                235.69                    0.47                222.23   \n",
       "422                204.70                    0.53                214.28   \n",
       "423                207.24                    0.51                216.75   \n",
       "424                230.99                    0.51                233.95   \n",
       "\n",
       "     ERCsSumAveragebaseline  ERCsSumVariancebaseline  \\\n",
       "0                     29.39                   640.73   \n",
       "1                     28.37                   652.46   \n",
       "2                     29.95                   644.68   \n",
       "3                     29.07                   654.06   \n",
       "4                     29.23                   696.61   \n",
       "..                      ...                      ...   \n",
       "420                   25.64                   630.00   \n",
       "421                   29.77                   653.24   \n",
       "422                   24.44                   652.40   \n",
       "423                   29.69                   659.77   \n",
       "424                   32.46                   704.80   \n",
       "\n",
       "     ERCsClusterShadebaseline  ERCs_thicknessbaseline  ERCsVolumebaseline  \\\n",
       "0                       57.73                    2.53              1278.0   \n",
       "1                     2072.42                    2.61              1027.0   \n",
       "2                     -467.36                    2.45              1819.0   \n",
       "3                     1421.62                    3.48              2002.0   \n",
       "4                     -507.05                    3.36              1945.0   \n",
       "..                        ...                     ...                 ...   \n",
       "420                  -2180.39                    2.38              1199.0   \n",
       "421                   1565.56                    2.71              1446.0   \n",
       "422                  -3077.22                    2.33              1327.0   \n",
       "423                   1771.64                    2.81              1801.0   \n",
       "424                   2099.77                    2.67              2063.0   \n",
       "\n",
       "     HipposcampusVolumebaseline  \n",
       "0                        2448.0  \n",
       "1                        2349.0  \n",
       "2                        3631.0  \n",
       "3                        3400.0  \n",
       "4                        4210.0  \n",
       "..                          ...  \n",
       "420                      2999.0  \n",
       "421                      3163.0  \n",
       "422                      2918.0  \n",
       "423                      3543.0  \n",
       "424                      3681.0  \n",
       "\n",
       "[425 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_data = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/datasets/processed/filtered_features/all_groups/train_processed.csv\")\n",
    "X_test_data = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/datasets/processed/filtered_features/all_groups/test_processed.csv\")\n",
    "\n",
    "y_train = X_train_data[\"Diagnosis\"]\n",
    "y_test = X_test_data[\"Diagnosis\"]\n",
    "X_train = X_train_data.drop(\"Diagnosis\", axis=1)\n",
    "X_test = X_test_data.drop(\"Diagnosis\", axis=1)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((425, 22), (183, 22))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate into training and testing set\n",
    "dataset.drop(labels = \"RID\", axis = 1, inplace = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset.drop(\"Diagnosis\", axis=1),  \n",
    "    dataset[\"Diagnosis\"],  \n",
    "    test_size=0.3,  \n",
    "    random_state=0,  \n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Data imputation & feature scaling\n",
    "\n",
    "Lets extract all columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MMSE0m', 'HipsASMbaseline', 'HipsContrastbaseline',\n",
       "       'HipsCorelationbaseline', 'HipsVariancebaseline',\n",
       "       'HipsSumAveragebaseline', 'HipsSumVariancebaseline',\n",
       "       'HipsEntropybaseline', 'HipsClusterShadebaseline', 'ERCsASMbaseline',\n",
       "       'ERCsContrastbaseline', 'ERCsCorelationbaseline',\n",
       "       'ERCsVariancebaseline', 'ERCsSumAveragebaseline',\n",
       "       'ERCsSumVariancebaseline', 'ERCsEntropybaseline',\n",
       "       'ERCsClusterShadebaseline', 'ERCs_thicknessbaseline',\n",
       "       'ERCsVolumebaseline', 'HipposcampusVolumebaseline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_columns = dataset.columns[dataset.isnull().sum() > 0]\n",
    "na_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"imputer\", MeanMedianImputer(\n",
    "        imputation_method=\"mean\", \n",
    "        variables=[\n",
    "            'MMSE0m', 'HipsASMbaseline', 'HipsContrastbaseline',\n",
    "            'HipsCorelationbaseline', 'HipsVariancebaseline',\n",
    "            'HipsSumAveragebaseline', 'HipsSumVariancebaseline',\n",
    "            'HipsEntropybaseline', 'HipsClusterShadebaseline', \n",
    "            'ERCsASMbaseline', 'ERCsContrastbaseline', \n",
    "            'ERCsCorelationbaseline', 'ERCsVariancebaseline', \n",
    "            'ERCsSumAveragebaseline', 'ERCsSumVariancebaseline',\n",
    "            'ERCsEntropybaseline', 'ERCsClusterShadebaseline', \n",
    "            'ERCs_thicknessbaseline', 'ERCsVolumebaseline', \n",
    "            'HipposcampusVolumebaseline'\n",
    "        ]\n",
    "    )),\n",
    "    (\"scaler\", StandardScaler().set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# let's transform the data with the pipeline\n",
    "X_train_scaled = pipe.transform(X_train)\n",
    "X_test_scaled = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "* Typically, after imputing the dataset, we analyze and visualize the data to assess whether it has been significantly affected. However, for the sake of simplicity, we will temporarily skip this step.\n",
    "* A split analysis will be conducted after the first iteration.\n",
    "* Additionally, no feature selection will be performed until we complete the first iteration.\n",
    "* Variables have not been dropped.\n",
    "* Hyperparameter optimization has not been performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We will experiment with various models that were previously mentioned in paper. \n",
    "\n",
    "* Logistic regression\n",
    "* Support vector machine\n",
    "* Decision tree\n",
    "* Random forest\n",
    "\n",
    "I will only focus on these 4 models for now. Though i would love to check how a simple ANN would work here. I'll try that afterwards.\n",
    "\n",
    "Documentation on sklearn for any of the below models\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "lg = LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\", max_iter = 1000, random_state = 42)\n",
    "\n",
    "svm = SVC(kernel ='rbf', decision_function_shape ='ovo', probability = True, random_state = 42)\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion ='gini', max_depth = 5, min_samples_split = 10, \n",
    "                            min_samples_leaf = 5, max_features = 'sqrt', random_state = 42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = 5, min_samples_split = 10, \n",
    "                            min_samples_leaf = 5, max_features = 'sqrt', bootstrap = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, it was stated that they performed 5 KFolds, so we will replicate their approach. \n",
    "\n",
    "ROC AUC along with other mentioned metrics will be covered here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "# Define metrics to evaluate\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average = 'weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average = 'weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average = 'weighted', zero_division=0),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', response_method = \"predict_proba\")\n",
    "}\n",
    "\n",
    "models  = {\"Logistic Regression\": lg, \n",
    "           \"Support Vector Machine\": svm, \n",
    "           \"Decision Tree\": dt, \n",
    "           \"Random Forest\": rf\n",
    "}\n",
    "\n",
    "model_data_mapping = {\n",
    "    'Logistic Regression': X_train_scaled,\n",
    "    'Support Vector Machine': X_train_scaled,\n",
    "    'Decision Tree': X_train,\n",
    "    'Random Forest': X_train\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Logistic Regression\n",
      "Mean train accuracy: 0.7176470588235294 Â± 0.010685824779167581\n",
      "Mean test accuracy: 0.628235294117647 Â± 0.021820278812931068\n",
      "Mean train precision: 0.7137715549459933 Â± 0.009994613394817947\n",
      "Mean test precision: 0.6346294513584386 Â± 0.016375364642513202\n",
      "Mean train recall: 0.7176470588235294 Â± 0.010685824779167581\n",
      "Mean test recall: 0.628235294117647 Â± 0.021820278812931068\n",
      "Mean train f1: 0.7141321948311611 Â± 0.01008676011818872\n",
      "Mean test f1: 0.6264786751157274 Â± 0.018573953162226077\n",
      "Mean train roc_auc: 0.9092337235320909 Â± 0.005358305500420103\n",
      "Mean test roc_auc: 0.8598573511351006 Â± 0.021712265716621503\n",
      "------------------------------------------------------\n",
      "Support Vector Machine\n",
      "Mean train accuracy: 0.8023529411764706 Â± 0.005060191333554468\n",
      "Mean test accuracy: 0.5999999999999999 Â± 0.034899757586332535\n",
      "Mean train precision: 0.8076762897810934 Â± 0.004279107619041377\n",
      "Mean test precision: 0.5951572225624752 Â± 0.06058994372992349\n",
      "Mean train recall: 0.8023529411764706 Â± 0.005060191333554468\n",
      "Mean test recall: 0.5999999999999999 Â± 0.034899757586332535\n",
      "Mean train f1: 0.7902525236346591 Â± 0.010861552429417795\n",
      "Mean test f1: 0.5822790396514332 Â± 0.03686349174882935\n",
      "Mean train roc_auc: 0.961066882246229 Â± 0.002089761767459735\n",
      "Mean test roc_auc: 0.8334618581934207 Â± 0.01947680760974415\n",
      "------------------------------------------------------\n",
      "Decision Tree\n",
      "Mean train accuracy: 0.6611764705882353 Â± 0.015607646072260716\n",
      "Mean test accuracy: 0.48470588235294115 Â± 0.03965246952082993\n",
      "Mean train precision: 0.6796646938094781 Â± 0.02293938320519404\n",
      "Mean test precision: 0.5226249600578503 Â± 0.0779000047738367\n",
      "Mean train recall: 0.6611764705882353 Â± 0.015607646072260716\n",
      "Mean test recall: 0.48470588235294115 Â± 0.03965246952082993\n",
      "Mean train f1: 0.6570135443289133 Â± 0.02069161904344594\n",
      "Mean test f1: 0.4885023003135574 Â± 0.04736948595444704\n",
      "Mean train roc_auc: 0.8739298010448676 Â± 0.011918136463410983\n",
      "Mean test roc_auc: 0.6833152973377563 Â± 0.04786030153999176\n",
      "------------------------------------------------------\n",
      "Random Forest\n",
      "Mean train accuracy: 0.8288235294117646 Â± 0.007979211744853274\n",
      "Mean test accuracy: 0.6329411764705882 Â± 0.02919923210821376\n",
      "Mean train precision: 0.8449432677220934 Â± 0.005132214027408043\n",
      "Mean test precision: 0.6291677537030343 Â± 0.04999119327336689\n",
      "Mean train recall: 0.8288235294117646 Â± 0.007979211744853274\n",
      "Mean test recall: 0.6329411764705882 Â± 0.02919923210821376\n",
      "Mean train f1: 0.8178129408867527 Â± 0.012721745820971941\n",
      "Mean test f1: 0.6014549066181137 Â± 0.0322475963166305\n",
      "Mean train roc_auc: 0.9784966965143799 Â± 0.0019831194312036346\n",
      "Mean test roc_auc: 0.8467384070128336 Â± 0.011805596191024063\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "\n",
    "    X_train_to_use = model_data_mapping[model_name]\n",
    "    \n",
    "    results = cross_validate(model, \n",
    "                             X_train_to_use, \n",
    "                             y_train, \n",
    "                             scoring = scoring_metrics,\n",
    "                             return_train_score = True,\n",
    "                             cv = kf)\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(model_name)\n",
    "    for metric in scoring_metrics.keys():\n",
    "            print(f'Mean train {metric}:', np.mean(results[f'train_{metric}']), 'Â±', np.std(results[f'train_{metric}']))\n",
    "            print(f'Mean test {metric}:', np.mean(results[f'test_{metric}']), 'Â±', np.std(results[f'test_{metric}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that based on the accuracy metric only, Random Forest can be considered as the best model with a mean test accuracy of 0.6329.\n",
    "While the worst Model would be Decision Tree, with a mean test accuracy of 0.4847."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
