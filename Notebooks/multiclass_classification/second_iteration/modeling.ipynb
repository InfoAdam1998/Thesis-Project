{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Import dataset\n",
    "X_train = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/Datasets/processed/X_train_processed.csv\")\n",
    "X_test = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/Datasets/processed/X_test_processed.csv\")\n",
    "y_train = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/Datasets/processed/y_train_processed.csv\")\n",
    "y_test = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/Datasets/processed/y_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((425, 16), (183, 16), (425,), (183,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "# let's transform the data \n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "X_train_scaled.shape, X_test_scaled.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Data imputation & feature scaling\n",
    "\n",
    "Lets extract all columns with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "* Typically, after imputing the dataset, we analyze and visualize the data to assess whether it has been significantly affected. However, for the sake of simplicity, we will temporarily skip this step.\n",
    "* A split analysis will be conducted after the first iteration.\n",
    "* Additionally, no feature selection will be performed until we complete the first iteration.\n",
    "* Variables have not been dropped.\n",
    "* Hyperparameter optimization has not been performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We will experiment with various models that were previously mentioned in paper. \n",
    "\n",
    "* Logistic regression\n",
    "* Support vector machine\n",
    "* Decision tree\n",
    "* Random forest\n",
    "\n",
    "I will only focus on these 4 models for now. Though i would love to check how a simple ANN would work here. I'll try that afterwards.\n",
    "\n",
    "Documentation on sklearn for any of the below models\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "lg = LogisticRegression(multi_class = \"multinomial\", solver = \"lbfgs\", max_iter = 1000, random_state = 42)\n",
    "\n",
    "svm = SVC(kernel ='rbf', decision_function_shape ='ovo', probability = True, random_state = 42)\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion ='gini', max_depth = 5, min_samples_split = 10, \n",
    "                            min_samples_leaf = 5, max_features = 'sqrt', random_state = 42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = 5, min_samples_split = 10, \n",
    "                            min_samples_leaf = 5, max_features = 'sqrt', bootstrap = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, it was stated that they performed 5 KFolds, so we will replicate their approach. \n",
    "\n",
    "ROC AUC along with other mentioned metrics will be covered here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "# Define metrics to evaluate\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average = 'weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average = 'weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average = 'weighted', zero_division=0),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', response_method = \"predict_proba\")\n",
    "}\n",
    "\n",
    "models  = {\"Logistic Regression\": lg, \n",
    "           \"Support Vector Machine\": svm, \n",
    "           \"Decision Tree\": dt, \n",
    "           \"Random Forest\": rf\n",
    "}\n",
    "\n",
    "model_data_mapping = {\n",
    "    'Logistic Regression': X_train_scaled,\n",
    "    'Support Vector Machine': X_train_scaled,\n",
    "    'Decision Tree': X_train,\n",
    "    'Random Forest': X_train\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Logistic Regression\n",
      "Mean train accuracy: 0.6941176470588235 ± 0.01146681687624584\n",
      "Mean test accuracy: 0.64 ± 0.03121529214452141\n",
      "Mean train precision: 0.6871284262244898 ± 0.012349662904269539\n",
      "Mean test precision: 0.6531650575161011 ± 0.018219520864726575\n",
      "Mean train recall: 0.6941176470588235 ± 0.01146681687624584\n",
      "Mean test recall: 0.64 ± 0.03121529214452141\n",
      "Mean train f1: 0.6875366459443062 ± 0.011608383632557891\n",
      "Mean test f1: 0.6393745655926393 ± 0.027455935848831044\n",
      "Mean train roc_auc: 0.8920854430888395 ± 0.004100231014048017\n",
      "Mean test roc_auc: 0.8532891781281544 ± 0.01923201734459709\n",
      "------------------------------------------------------\n",
      "Support Vector Machine\n",
      "Mean train accuracy: 0.7752941176470587 ± 0.009593827311941234\n",
      "Mean test accuracy: 0.583529411764706 ± 0.038375309247764895\n",
      "Mean train precision: 0.7749055251185821 ± 0.010843378163222764\n",
      "Mean test precision: 0.5800207410258946 ± 0.031212381300747563\n",
      "Mean train recall: 0.7752941176470587 ± 0.009593827311941234\n",
      "Mean test recall: 0.583529411764706 ± 0.038375309247764895\n",
      "Mean train f1: 0.766803545357318 ± 0.009690197203232474\n",
      "Mean test f1: 0.5691442896726835 ± 0.03606513035114108\n",
      "Mean train roc_auc: 0.9466772901056224 ± 0.0049362878820188435\n",
      "Mean test roc_auc: 0.8235532255241067 ± 0.016976477526641453\n",
      "------------------------------------------------------\n",
      "Decision Tree\n",
      "Mean train accuracy: 0.7005882352941176 ± 0.0158277929965573\n",
      "Mean test accuracy: 0.4988235294117647 ± 0.05292810296095055\n",
      "Mean train precision: 0.6959359884166603 ± 0.017541150550336504\n",
      "Mean test precision: 0.5029860465849751 ± 0.07862160642862086\n",
      "Mean train recall: 0.7005882352941176 ± 0.0158277929965573\n",
      "Mean test recall: 0.4988235294117647 ± 0.05292810296095055\n",
      "Mean train f1: 0.6811766023429656 ± 0.01960337050884977\n",
      "Mean test f1: 0.4760410026044801 ± 0.05625859696676388\n",
      "Mean train roc_auc: 0.8900445471571429 ± 0.013357949750404774\n",
      "Mean test roc_auc: 0.7434003575258192 ± 0.031617495683734\n",
      "------------------------------------------------------\n",
      "Random Forest\n",
      "Mean train accuracy: 0.8070588235294117 ± 0.01551871289208578\n",
      "Mean test accuracy: 0.6070588235294119 ± 0.03614656822526404\n",
      "Mean train precision: 0.8224642737350333 ± 0.01632202609883332\n",
      "Mean test precision: 0.599062218206253 ± 0.030309259206173195\n",
      "Mean train recall: 0.8070588235294117 ± 0.01551871289208578\n",
      "Mean test recall: 0.6070588235294119 ± 0.03614656822526404\n",
      "Mean train f1: 0.795473180072406 ± 0.018022300017848804\n",
      "Mean test f1: 0.581026339235055 ± 0.02435766322697153\n",
      "Mean train roc_auc: 0.9690488403559815 ± 0.0035312255615683047\n",
      "Mean test roc_auc: 0.8301850542619891 ± 0.0168505115492999\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "\n",
    "    X_train_to_use = model_data_mapping[model_name]\n",
    "    \n",
    "    results = cross_validate(model, \n",
    "                             X_train_to_use, \n",
    "                             y_train, \n",
    "                             scoring = scoring_metrics,\n",
    "                             return_train_score = True,\n",
    "                             cv = kf)\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(model_name)\n",
    "    for metric in scoring_metrics.keys():\n",
    "            print(f'Mean train {metric}:', np.mean(results[f'train_{metric}']), '±', np.std(results[f'train_{metric}']))\n",
    "            print(f'Mean test {metric}:', np.mean(results[f'test_{metric}']), '±', np.std(results[f'test_{metric}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that based on the accuracy metric only, Random Forest can be considered as the best model with a mean test accuracy of 0.6329.\n",
    "While the worst Model would be Decision Tree, with a mean test accuracy of 0.4847."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
