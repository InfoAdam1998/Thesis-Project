{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ageatscreening</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>MMSE0m</th>\n",
       "      <th>HipsASMbaseline</th>\n",
       "      <th>HipsContrastbaseline</th>\n",
       "      <th>HipsCorelationbaseline</th>\n",
       "      <th>HipsVariancebaseline</th>\n",
       "      <th>HipsSumAveragebaseline</th>\n",
       "      <th>...</th>\n",
       "      <th>ERCsContrastbaseline</th>\n",
       "      <th>ERCsCorelationbaseline</th>\n",
       "      <th>ERCsVariancebaseline</th>\n",
       "      <th>ERCsSumAveragebaseline</th>\n",
       "      <th>ERCsSumVariancebaseline</th>\n",
       "      <th>ERCsEntropybaseline</th>\n",
       "      <th>ERCsClusterShadebaseline</th>\n",
       "      <th>ERCs_thicknessbaseline</th>\n",
       "      <th>ERCsVolumebaseline</th>\n",
       "      <th>HipposcampusVolumebaseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>81.3479</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158.27</td>\n",
       "      <td>0.63</td>\n",
       "      <td>218.30</td>\n",
       "      <td>28.37</td>\n",
       "      <td>...</td>\n",
       "      <td>253.10</td>\n",
       "      <td>0.40</td>\n",
       "      <td>208.65</td>\n",
       "      <td>23.39</td>\n",
       "      <td>581.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2568.19</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>3047.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>67.6904</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>147.64</td>\n",
       "      <td>0.55</td>\n",
       "      <td>173.64</td>\n",
       "      <td>44.72</td>\n",
       "      <td>...</td>\n",
       "      <td>220.88</td>\n",
       "      <td>0.48</td>\n",
       "      <td>215.70</td>\n",
       "      <td>33.74</td>\n",
       "      <td>641.90</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4113.01</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>3449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>73.8027</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>199.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>222.27</td>\n",
       "      <td>41.18</td>\n",
       "      <td>...</td>\n",
       "      <td>220.37</td>\n",
       "      <td>0.54</td>\n",
       "      <td>232.18</td>\n",
       "      <td>29.18</td>\n",
       "      <td>708.36</td>\n",
       "      <td>2.87</td>\n",
       "      <td>-1388.41</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2044.0</td>\n",
       "      <td>3441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>84.5945</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>184.21</td>\n",
       "      <td>0.53</td>\n",
       "      <td>201.55</td>\n",
       "      <td>43.04</td>\n",
       "      <td>...</td>\n",
       "      <td>198.42</td>\n",
       "      <td>0.54</td>\n",
       "      <td>220.48</td>\n",
       "      <td>26.68</td>\n",
       "      <td>683.50</td>\n",
       "      <td>2.77</td>\n",
       "      <td>-2506.55</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>2875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>73.9726</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>233.02</td>\n",
       "      <td>0.48</td>\n",
       "      <td>229.88</td>\n",
       "      <td>39.46</td>\n",
       "      <td>...</td>\n",
       "      <td>196.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>210.63</td>\n",
       "      <td>26.60</td>\n",
       "      <td>645.95</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-1164.02</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1397.0</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RID  Gender  Ageatscreening  Diagnosis  MMSE0m  HipsASMbaseline  \\\n",
       "0    3       0         81.3479          3    20.0              NaN   \n",
       "1    4       0         67.6904          1    27.0             0.06   \n",
       "2    5       0         73.8027          0    29.0             0.10   \n",
       "3    8       1         84.5945          0    28.0             0.08   \n",
       "4   10       1         73.9726          3    24.0             0.11   \n",
       "\n",
       "   HipsContrastbaseline  HipsCorelationbaseline  HipsVariancebaseline  \\\n",
       "0                158.27                    0.63                218.30   \n",
       "1                147.64                    0.55                173.64   \n",
       "2                199.66                    0.55                222.27   \n",
       "3                184.21                    0.53                201.55   \n",
       "4                233.02                    0.48                229.88   \n",
       "\n",
       "   HipsSumAveragebaseline  ...  ERCsContrastbaseline  ERCsCorelationbaseline  \\\n",
       "0                   28.37  ...                253.10                    0.40   \n",
       "1                   44.72  ...                220.88                    0.48   \n",
       "2                   41.18  ...                220.37                    0.54   \n",
       "3                   43.04  ...                198.42                    0.54   \n",
       "4                   39.46  ...                196.55                    0.53   \n",
       "\n",
       "   ERCsVariancebaseline  ERCsSumAveragebaseline  ERCsSumVariancebaseline  \\\n",
       "0                208.65                   23.39                   581.50   \n",
       "1                215.70                   33.74                   641.90   \n",
       "2                232.18                   29.18                   708.36   \n",
       "3                220.48                   26.68                   683.50   \n",
       "4                210.63                   26.60                   645.95   \n",
       "\n",
       "   ERCsEntropybaseline  ERCsClusterShadebaseline  ERCs_thicknessbaseline  \\\n",
       "0                  NaN                  -2568.19                    2.31   \n",
       "1                 3.33                   4113.01                    2.76   \n",
       "2                 2.87                  -1388.41                    3.18   \n",
       "3                 2.77                  -2506.55                    2.68   \n",
       "4                 2.72                  -1164.02                    2.64   \n",
       "\n",
       "   ERCsVolumebaseline  HipposcampusVolumebaseline  \n",
       "0              1176.0                      3047.0  \n",
       "1              1942.0                      3449.0  \n",
       "2              2044.0                      3441.0  \n",
       "3              1959.0                      2875.0  \n",
       "4              1397.0                      2700.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/ADNI(Rawdata).csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group one\n",
    "group_one = dataset[dataset[\"Diagnosis\"] == 0]\n",
    "group_two = dataset[dataset[\"Diagnosis\"] == 3]\n",
    "\n",
    "combined_groups_one = pd.concat([group_one, group_two], ignore_index = True)\n",
    "combined_groups_one[\"Diagnosis\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((226, 22), (98, 22))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate into training and testing set\n",
    "combined_groups_one.drop(labels = \"RID\", axis = 1, inplace = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined_groups_one.drop(\"Diagnosis\", axis=1),  \n",
    "    combined_groups_one[\"Diagnosis\"],  \n",
    "    test_size=0.3,  \n",
    "    random_state=0,  \n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"imputer\", MeanMedianImputer(\n",
    "        imputation_method=\"mean\", \n",
    "        variables=[\n",
    "            'MMSE0m', 'HipsASMbaseline', 'HipsContrastbaseline',\n",
    "            'HipsCorelationbaseline', 'HipsVariancebaseline',\n",
    "            'HipsSumAveragebaseline', 'HipsSumVariancebaseline',\n",
    "            'HipsEntropybaseline', 'HipsClusterShadebaseline', \n",
    "            'ERCsASMbaseline', 'ERCsContrastbaseline', \n",
    "            'ERCsCorelationbaseline', 'ERCsVariancebaseline', \n",
    "            'ERCsSumAveragebaseline', 'ERCsSumVariancebaseline',\n",
    "            'ERCsEntropybaseline', 'ERCsClusterShadebaseline', \n",
    "            'ERCs_thicknessbaseline', 'ERCsVolumebaseline', \n",
    "            'HipposcampusVolumebaseline'\n",
    "        ]\n",
    "    )),\n",
    "    (\"scaler\", StandardScaler().set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# let's transform the data with the pipeline\n",
    "X_train_scaled = pipe.transform(X_train)\n",
    "X_test_scaled = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "lg = LogisticRegression(multi_class = \"multinomial\", \n",
    "                        solver = \"lbfgs\",\n",
    "                        max_iter = 1000,\n",
    "                        random_state = 42)\n",
    "\n",
    "svm = SVC(kernel ='rbf', \n",
    "          decision_function_shape ='ovo',\n",
    "          probability = True,\n",
    "          random_state = 42)\n",
    "\n",
    "dt = decision_tree_model = DecisionTreeClassifier(\n",
    "     criterion ='gini',      \n",
    "     max_depth = 5,           \n",
    "     min_samples_split = 10,  \n",
    "     min_samples_leaf = 5,    \n",
    "     max_features = 'sqrt',    \n",
    "     random_state = 42)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "     n_estimators = 100,     \n",
    "     criterion = 'gini',     \n",
    "     max_depth = 5,           \n",
    "     min_samples_split = 10, \n",
    "     min_samples_leaf = 5,   \n",
    "     max_features = 'sqrt',   \n",
    "     bootstrap = True,        \n",
    "     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "kf = KFold(n_splits = 5, \n",
    "           shuffle = True, \n",
    "           random_state = 42)\n",
    "\n",
    "# Define metrics to evaluate\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average = 'weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average = 'weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average = 'weighted', zero_division=0),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', response_method = \"predict_proba\")\n",
    "}\n",
    "\n",
    "lg_results = cross_validate(\n",
    "    lg,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "svm_results = cross_validate(\n",
    "    svm,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "dt_results = cross_validate(\n",
    "    dt,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "rf_results = cross_validate(\n",
    "    rf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Logistic Regression\n",
      "Mean train set accuracy: 0.9878207489257212 ± 0.00648101625490923\n",
      "Mean test set accuracy: 0.9733333333333334 ± 0.025915341754867992\n",
      "Mean train precision: 0.9878909868918562 ± 0.006505511027715961\n",
      "Mean test precision: 0.9742192118226601 ± 0.025498790619537207\n",
      "Mean train recall: 0.9878207489257212 ± 0.00648101625490923\n",
      "Mean test recall: 0.9733333333333334 ± 0.025915341754867992\n",
      "Mean train F1 score: 0.9878124699527777 ± 0.0064801572374334\n",
      "Mean test F1 score: 0.9731793747411223 ± 0.02610560531948763\n",
      "Mean train ROC AUC: 0.9996731189686431 ± 0.00015291827895117462\n",
      "Mean test ROC AUC: 0.9954393903310311 ± 0.005536149089445782 \n",
      "\n",
      "------------------------------------------------------\n",
      "Support Vector Machine\n",
      "Mean train set accuracy: 0.9822958870472682 ± 0.004160885404953817\n",
      "Mean test set accuracy: 0.9556521739130435 ± 0.0344893704615623\n",
      "Mean train precision: 0.9828497876300352 ± 0.003930861218971975\n",
      "Mean test precision: 0.9604295544468862 ± 0.027952734104091628\n",
      "Mean train recall: 0.9822958870472682 ± 0.004160885404953817\n",
      "Mean test recall: 0.9556521739130435 ± 0.0344893704615623\n",
      "Mean train F1 score: 0.9822357306164304 ± 0.004191135984438851\n",
      "Mean test F1 score: 0.9548258224200763 ± 0.03586660941819146\n",
      "Mean train ROC AUC: 0.998715620967317 ± 0.0005424939370163326\n",
      "Mean test ROC AUC: 0.9854931446262715 ± 0.01749605512338818 \n",
      "\n",
      "------------------------------------------------------\n",
      "Decision Tree\n",
      "Mean train set accuracy: 0.9125782688766113 ± 0.024211785741651188\n",
      "Mean test set accuracy: 0.8315942028985507 ± 0.03410176373867557\n",
      "Mean train precision: 0.913093441888748 ± 0.024249901217482342\n",
      "Mean test precision: 0.8392970124968905 ± 0.03141996492698017\n",
      "Mean train recall: 0.9125782688766113 ± 0.024211785741651188\n",
      "Mean test recall: 0.8315942028985507 ± 0.03410176373867557\n",
      "Mean train F1 score: 0.9126157396830781 ± 0.024217149270825043\n",
      "Mean test F1 score: 0.8315911938073182 ± 0.032925780099693956\n",
      "Mean train ROC AUC: 0.9661654475935834 ± 0.022520152680247103\n",
      "Mean test ROC AUC: 0.9036817735927644 ± 0.025037128971248577 \n",
      "\n",
      "------------------------------------------------------\n",
      "Random Forest\n",
      "Mean train set accuracy: 0.9922529158993247 ± 0.004429156100040546\n",
      "Mean test set accuracy: 0.9555555555555555 ± 0.028109134757052262\n",
      "Mean train precision: 0.992266728054021 ± 0.004436166612651143\n",
      "Mean test precision: 0.9588615984405457 ± 0.024514580202240625\n",
      "Mean train recall: 0.9922529158993247 ± 0.004429156100040546\n",
      "Mean test recall: 0.9555555555555555 ± 0.028109134757052262\n",
      "Mean train F1 score: 0.9922536690483866 ± 0.004429534214409422\n",
      "Mean test F1 score: 0.9552270290832382 ± 0.028764763494592015\n",
      "Mean train ROC AUC: 0.9995733738391139 ± 0.00031175741300132424\n",
      "Mean test ROC AUC: 0.9954087707957677 ± 0.007215185703235442 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results for Logistic Regression\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Logistic Regression\")\n",
    "print('Mean train set accuracy:', np.mean(lg_results['train_accuracy']), '±', np.std(lg_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(lg_results['test_accuracy']), '±', np.std(lg_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(lg_results['train_precision']), '±', np.std(lg_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(lg_results['test_precision']), '±', np.std(lg_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(lg_results['train_recall']), '±', np.std(lg_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(lg_results['test_recall']), '±', np.std(lg_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(lg_results['train_f1']), '±', np.std(lg_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(lg_results['test_f1']), '±', np.std(lg_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(lg_results['train_roc_auc']), '±', np.std(lg_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(lg_results['test_roc_auc']), '±', np.std(lg_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "# Print results for Support Vector Machine\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Support Vector Machine\")\n",
    "print('Mean train set accuracy:', np.mean(svm_results['train_accuracy']), '±', np.std(svm_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(svm_results['test_accuracy']), '±', np.std(svm_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(svm_results['train_precision']), '±', np.std(svm_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(svm_results['test_precision']), '±', np.std(svm_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(svm_results['train_recall']), '±', np.std(svm_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(svm_results['test_recall']), '±', np.std(svm_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(svm_results['train_f1']), '±', np.std(svm_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(svm_results['test_f1']), '±', np.std(svm_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(svm_results['train_roc_auc']), '±', np.std(svm_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(svm_results['test_roc_auc']), '±', np.std(svm_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "# Print results for Decision Tree\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Decision Tree\")\n",
    "print('Mean train set accuracy:', np.mean(dt_results['train_accuracy']), '±', np.std(dt_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(dt_results['test_accuracy']), '±', np.std(dt_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(dt_results['train_precision']), '±', np.std(dt_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(dt_results['test_precision']), '±', np.std(dt_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(dt_results['train_recall']), '±', np.std(dt_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(dt_results['test_recall']), '±', np.std(dt_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(dt_results['train_f1']), '±', np.std(dt_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(dt_results['test_f1']), '±', np.std(dt_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(dt_results['train_roc_auc']), '±', np.std(dt_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(dt_results['test_roc_auc']), '±', np.std(dt_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "# Print results for Random Forest\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Random Forest\")\n",
    "print('Mean train set accuracy:', np.mean(rf_results['train_accuracy']), '±', np.std(rf_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(rf_results['test_accuracy']), '±', np.std(rf_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(rf_results['train_precision']), '±', np.std(rf_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(rf_results['test_precision']), '±', np.std(rf_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(rf_results['train_recall']), '±', np.std(rf_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(rf_results['test_recall']), '±', np.std(rf_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(rf_results['train_f1']), '±', np.std(rf_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(rf_results['test_f1']), '±', np.std(rf_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(rf_results['train_roc_auc']), '±', np.std(rf_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(rf_results['test_roc_auc']), '±', np.std(rf_results['test_roc_auc']), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
