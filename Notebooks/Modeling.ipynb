{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "* It is good practice to split the data prior to augmenting it\n",
    "* A pipeline will be created on the second iteration.\n",
    "* Ill drop the RID for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ageatscreening</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>MMSE0m</th>\n",
       "      <th>HipsASMbaseline</th>\n",
       "      <th>HipsContrastbaseline</th>\n",
       "      <th>HipsCorelationbaseline</th>\n",
       "      <th>HipsVariancebaseline</th>\n",
       "      <th>HipsSumAveragebaseline</th>\n",
       "      <th>...</th>\n",
       "      <th>ERCsContrastbaseline</th>\n",
       "      <th>ERCsCorelationbaseline</th>\n",
       "      <th>ERCsVariancebaseline</th>\n",
       "      <th>ERCsSumAveragebaseline</th>\n",
       "      <th>ERCsSumVariancebaseline</th>\n",
       "      <th>ERCsEntropybaseline</th>\n",
       "      <th>ERCsClusterShadebaseline</th>\n",
       "      <th>ERCs_thicknessbaseline</th>\n",
       "      <th>ERCsVolumebaseline</th>\n",
       "      <th>HipposcampusVolumebaseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>81.3479</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158.27</td>\n",
       "      <td>0.63</td>\n",
       "      <td>218.30</td>\n",
       "      <td>28.37</td>\n",
       "      <td>...</td>\n",
       "      <td>253.10</td>\n",
       "      <td>0.40</td>\n",
       "      <td>208.65</td>\n",
       "      <td>23.39</td>\n",
       "      <td>581.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2568.19</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>3047.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>67.6904</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>147.64</td>\n",
       "      <td>0.55</td>\n",
       "      <td>173.64</td>\n",
       "      <td>44.72</td>\n",
       "      <td>...</td>\n",
       "      <td>220.88</td>\n",
       "      <td>0.48</td>\n",
       "      <td>215.70</td>\n",
       "      <td>33.74</td>\n",
       "      <td>641.90</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4113.01</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>3449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>73.8027</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>199.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>222.27</td>\n",
       "      <td>41.18</td>\n",
       "      <td>...</td>\n",
       "      <td>220.37</td>\n",
       "      <td>0.54</td>\n",
       "      <td>232.18</td>\n",
       "      <td>29.18</td>\n",
       "      <td>708.36</td>\n",
       "      <td>2.87</td>\n",
       "      <td>-1388.41</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2044.0</td>\n",
       "      <td>3441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>84.5945</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>184.21</td>\n",
       "      <td>0.53</td>\n",
       "      <td>201.55</td>\n",
       "      <td>43.04</td>\n",
       "      <td>...</td>\n",
       "      <td>198.42</td>\n",
       "      <td>0.54</td>\n",
       "      <td>220.48</td>\n",
       "      <td>26.68</td>\n",
       "      <td>683.50</td>\n",
       "      <td>2.77</td>\n",
       "      <td>-2506.55</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>2875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>73.9726</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>233.02</td>\n",
       "      <td>0.48</td>\n",
       "      <td>229.88</td>\n",
       "      <td>39.46</td>\n",
       "      <td>...</td>\n",
       "      <td>196.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>210.63</td>\n",
       "      <td>26.60</td>\n",
       "      <td>645.95</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-1164.02</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1397.0</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RID  Gender  Ageatscreening  Diagnosis  MMSE0m  HipsASMbaseline  \\\n",
       "0    3       0         81.3479          3    20.0              NaN   \n",
       "1    4       0         67.6904          1    27.0             0.06   \n",
       "2    5       0         73.8027          0    29.0             0.10   \n",
       "3    8       1         84.5945          0    28.0             0.08   \n",
       "4   10       1         73.9726          3    24.0             0.11   \n",
       "\n",
       "   HipsContrastbaseline  HipsCorelationbaseline  HipsVariancebaseline  \\\n",
       "0                158.27                    0.63                218.30   \n",
       "1                147.64                    0.55                173.64   \n",
       "2                199.66                    0.55                222.27   \n",
       "3                184.21                    0.53                201.55   \n",
       "4                233.02                    0.48                229.88   \n",
       "\n",
       "   HipsSumAveragebaseline  ...  ERCsContrastbaseline  ERCsCorelationbaseline  \\\n",
       "0                   28.37  ...                253.10                    0.40   \n",
       "1                   44.72  ...                220.88                    0.48   \n",
       "2                   41.18  ...                220.37                    0.54   \n",
       "3                   43.04  ...                198.42                    0.54   \n",
       "4                   39.46  ...                196.55                    0.53   \n",
       "\n",
       "   ERCsVariancebaseline  ERCsSumAveragebaseline  ERCsSumVariancebaseline  \\\n",
       "0                208.65                   23.39                   581.50   \n",
       "1                215.70                   33.74                   641.90   \n",
       "2                232.18                   29.18                   708.36   \n",
       "3                220.48                   26.68                   683.50   \n",
       "4                210.63                   26.60                   645.95   \n",
       "\n",
       "   ERCsEntropybaseline  ERCsClusterShadebaseline  ERCs_thicknessbaseline  \\\n",
       "0                  NaN                  -2568.19                    2.31   \n",
       "1                 3.33                   4113.01                    2.76   \n",
       "2                 2.87                  -1388.41                    3.18   \n",
       "3                 2.77                  -2506.55                    2.68   \n",
       "4                 2.72                  -1164.02                    2.64   \n",
       "\n",
       "   ERCsVolumebaseline  HipposcampusVolumebaseline  \n",
       "0              1176.0                      3047.0  \n",
       "1              1942.0                      3449.0  \n",
       "2              2044.0                      3441.0  \n",
       "3              1959.0                      2875.0  \n",
       "4              1397.0                      2700.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/ADNI(Rawdata).csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((425, 23), (183, 23))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset.drop(\"Diagnosis\", axis=1),  \n",
    "    dataset[\"Diagnosis\"],  \n",
    "    test_size=0.3,  \n",
    "    random_state=0,  \n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Data imputation & feature scaling\n",
    "\n",
    "Lets extract all columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MMSE0m', 'HipsASMbaseline', 'HipsContrastbaseline',\n",
       "       'HipsCorelationbaseline', 'HipsVariancebaseline',\n",
       "       'HipsSumAveragebaseline', 'HipsSumVariancebaseline',\n",
       "       'HipsEntropybaseline', 'HipsClusterShadebaseline', 'ERCsASMbaseline',\n",
       "       'ERCsContrastbaseline', 'ERCsCorelationbaseline',\n",
       "       'ERCsVariancebaseline', 'ERCsSumAveragebaseline',\n",
       "       'ERCsSumVariancebaseline', 'ERCsEntropybaseline',\n",
       "       'ERCsClusterShadebaseline', 'ERCs_thicknessbaseline',\n",
       "       'ERCsVolumebaseline', 'HipposcampusVolumebaseline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_columns = dataset.columns[dataset.isnull().sum() > 0]\n",
    "na_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"imputer\", MeanMedianImputer(\n",
    "        imputation_method=\"mean\", \n",
    "        variables=[\n",
    "            'MMSE0m', 'HipsASMbaseline', 'HipsContrastbaseline',\n",
    "            'HipsCorelationbaseline', 'HipsVariancebaseline',\n",
    "            'HipsSumAveragebaseline', 'HipsSumVariancebaseline',\n",
    "            'HipsEntropybaseline', 'HipsClusterShadebaseline', \n",
    "            'ERCsASMbaseline', 'ERCsContrastbaseline', \n",
    "            'ERCsCorelationbaseline', 'ERCsVariancebaseline', \n",
    "            'ERCsSumAveragebaseline', 'ERCsSumVariancebaseline',\n",
    "            'ERCsEntropybaseline', 'ERCsClusterShadebaseline', \n",
    "            'ERCs_thicknessbaseline', 'ERCsVolumebaseline', \n",
    "            'HipposcampusVolumebaseline'\n",
    "        ]\n",
    "    )),\n",
    "    (\"scaler\", StandardScaler().set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# let's transform the data with the pipeline\n",
    "X_train_scaled = pipe.transform(X_train)\n",
    "X_test_scaled = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "* Usually after imputing the dataset, we do an analysis and visualize the data if it has been affected greatly, but for simplicity sake, we will temporary ignore this step\n",
    "* Of course, a split analysis will be done after the first iteration\n",
    "* Also, no feature selection until we are done with first iteration\n",
    "* Didnt dropped variables\n",
    "* Didnt do hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We will experiment with various models that were previously mentioned in paper. \n",
    "\n",
    "* Logistic regression\n",
    "* Support vector machine\n",
    "* Decision tree\n",
    "* Random forest\n",
    "\n",
    "I will only focus on these 4 models for now. Though i would love to check how a simple ANN would work here. I'll try that afterwards.\n",
    "\n",
    "For simplicity sake, I will not do any hyper parameter optimization yet.\n",
    "\n",
    "Documentation on sklearn for any of the below models\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "lg = LogisticRegression(multi_class = \"multinomial\", \n",
    "                        solver = \"lbfgs\",\n",
    "                        max_iter = 1000,\n",
    "                        random_state = 42)\n",
    "\n",
    "svm = SVC(kernel ='rbf', \n",
    "          decision_function_shape ='ovo',\n",
    "          probability = True,\n",
    "          random_state = 42)\n",
    "\n",
    "dt = decision_tree_model = DecisionTreeClassifier(\n",
    "     criterion ='gini',      # 'gini' or 'entropy'\n",
    "     max_depth = 5,           # Set depth to prevent overfitting\n",
    "     min_samples_split = 10,  # Minimum samples required to split a node\n",
    "     min_samples_leaf = 5,    # Minimum samples required at a leaf node\n",
    "     max_features = 'sqrt',    # Use square root of features\n",
    "     random_state = 42)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "     n_estimators = 100,      # Number of trees in the forest\n",
    "     criterion = 'gini',      # 'gini' or 'entropy'\n",
    "     max_depth = 5,           # Set depth to prevent overfitting\n",
    "     min_samples_split = 10,  # Minimum samples required to split a node\n",
    "     min_samples_leaf = 5,    # Minimum samples required at a leaf node\n",
    "     max_features = 'sqrt',    # Use square root of features\n",
    "     bootstrap = True,        # Use bootstrap sampling\n",
    "     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Mean train set accuracy: 0.7276470588235295 Â± 0.013745084053585797\n",
      "Mean test set accuracy: 0.6423529411764706 Â± 0.038375309247764916\n",
      "Mean precision: 0.6516870783681201 Â± 0.03412220878151855\n",
      "Mean recall: 0.6423529411764706 Â± 0.038375309247764916\n",
      "Mean F1 score: 0.6399057838266943 Â± 0.03730497417504237\n",
      "Mean ROC AUC: 0.8614475185771248 Â± 0.022785370156711542 \n",
      "\n",
      "Support Vector Machine\n",
      "Mean train set accuracy: 0.8194117647058823 Â± 0.008442764761416076\n",
      "Mean test set accuracy: 0.6188235294117647 Â± 0.03690444033260735\n",
      "Mean precision: 0.6261895072083623 Â± 0.05357789052945798\n",
      "Mean recall: 0.6188235294117647 Â± 0.03690444033260735\n",
      "Mean F1 score: 0.601580918420586 Â± 0.03826163961470308\n",
      "Mean ROC AUC: 0.8386040607469702 Â± 0.022730705036446718 \n",
      "\n",
      "Decision Tree\n",
      "Mean train set accuracy: 0.6882352941176471 Â± 0.013542193450848648\n",
      "Mean test set accuracy: 0.5341176470588236 Â± 0.07609864295904109\n",
      "Mean precision: 0.5376986290470461 Â± 0.07457026661839881\n",
      "Mean recall: 0.5341176470588236 Â± 0.07609864295904109\n",
      "Mean F1 score: 0.5255311131547356 Â± 0.07804130751457576\n",
      "Mean ROC AUC: 0.7301409802562348 Â± 0.0436552868716777 \n",
      "\n",
      "Random Forest\n",
      "Mean train set accuracy: 0.8300000000000001 Â± 0.01729051583141065\n",
      "Mean test set accuracy: 0.6023529411764705 Â± 0.041024931233324013\n",
      "Mean precision: 0.612297112166171 Â± 0.06980597143981172\n",
      "Mean recall: 0.6023529411764705 Â± 0.041024931233324013\n",
      "Mean F1 score: 0.574525589021078 Â± 0.039879917785345186\n",
      "Mean ROC AUC: 0.8371626724830632 Â± 0.016219878693752715 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "kf = KFold(n_splits = 5, \n",
    "           shuffle = True, \n",
    "           random_state = 42)\n",
    "\n",
    "# Define metrics to evaluate\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average = 'weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average = 'weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average = 'weighted', zero_division=0),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', response_method = \"predict_proba\")\n",
    "}\n",
    "\n",
    "lg_results = cross_validate(\n",
    "    lg,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "svm_results = cross_validate(\n",
    "    svm,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "dt_results = cross_validate(\n",
    "    dt,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "rf_results = cross_validate(\n",
    "    rf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "# Print results for Logistic Regression\n",
    "print(\"Logistic Regression\")\n",
    "print('Mean train set accuracy:', np.mean(lg_results['train_accuracy']), 'Â±', np.std(lg_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(lg_results['test_accuracy']), 'Â±', np.std(lg_results['test_accuracy']))\n",
    "print('Mean precision:', np.mean(lg_results['test_precision']), 'Â±', np.std(lg_results['test_precision']))\n",
    "print('Mean recall:', np.mean(lg_results['test_recall']), 'Â±', np.std(lg_results['test_recall']))\n",
    "print('Mean F1 score:', np.mean(lg_results['test_f1']), 'Â±', np.std(lg_results['test_f1']))\n",
    "print('Mean ROC AUC:', np.mean(lg_results['test_roc_auc']), 'Â±', np.std(lg_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "# Repeat the print statements for SVM, Decision Tree, and Random Forest\n",
    "print(\"Support Vector Machine\")\n",
    "print('Mean train set accuracy:', np.mean(svm_results['train_accuracy']), 'Â±', np.std(svm_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(svm_results['test_accuracy']), 'Â±', np.std(svm_results['test_accuracy']))\n",
    "print('Mean precision:', np.mean(svm_results['test_precision']), 'Â±', np.std(svm_results['test_precision']))\n",
    "print('Mean recall:', np.mean(svm_results['test_recall']), 'Â±', np.std(svm_results['test_recall']))\n",
    "print('Mean F1 score:', np.mean(svm_results['test_f1']), 'Â±', np.std(svm_results['test_f1']))\n",
    "print('Mean ROC AUC:', np.mean(svm_results['test_roc_auc']), 'Â±', np.std(svm_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "print('Mean train set accuracy:', np.mean(dt_results['train_accuracy']), 'Â±', np.std(dt_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(dt_results['test_accuracy']), 'Â±', np.std(dt_results['test_accuracy']))\n",
    "print('Mean precision:', np.mean(dt_results['test_precision']), 'Â±', np.std(dt_results['test_precision']))\n",
    "print('Mean recall:', np.mean(dt_results['test_recall']), 'Â±', np.std(dt_results['test_recall']))\n",
    "print('Mean F1 score:', np.mean(dt_results['test_f1']), 'Â±', np.std(dt_results['test_f1']))\n",
    "print('Mean ROC AUC:', np.mean(dt_results['test_roc_auc']), 'Â±', np.std(dt_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "print(\"Random Forest\")\n",
    "print('Mean train set accuracy:', np.mean(rf_results['train_accuracy']), 'Â±', np.std(rf_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(rf_results['test_accuracy']), 'Â±', np.std(rf_results['test_accuracy']))\n",
    "print('Mean precision:', np.mean(rf_results['test_precision']), 'Â±', np.std(rf_results['test_precision']))\n",
    "print('Mean recall:', np.mean(rf_results['test_recall']), 'Â±', np.std(rf_results['test_recall']))\n",
    "print('Mean F1 score:', np.mean(rf_results['test_f1']), 'Â±', np.std(rf_results['test_f1']))\n",
    "print('Mean ROC AUC:', np.mean(rf_results['test_roc_auc']), 'Â±', np.std(rf_results['test_roc_auc']), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
