{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now that have covered the EDA phase, lets move to some simple feature engineering tasks prior to modeling.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "* It is good practice to split the data prior to augmenting it\n",
    "* Ill drop the RID column for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ageatscreening</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>MMSE0m</th>\n",
       "      <th>HipsASMbaseline</th>\n",
       "      <th>HipsContrastbaseline</th>\n",
       "      <th>HipsCorelationbaseline</th>\n",
       "      <th>HipsVariancebaseline</th>\n",
       "      <th>HipsSumAveragebaseline</th>\n",
       "      <th>...</th>\n",
       "      <th>ERCsContrastbaseline</th>\n",
       "      <th>ERCsCorelationbaseline</th>\n",
       "      <th>ERCsVariancebaseline</th>\n",
       "      <th>ERCsSumAveragebaseline</th>\n",
       "      <th>ERCsSumVariancebaseline</th>\n",
       "      <th>ERCsEntropybaseline</th>\n",
       "      <th>ERCsClusterShadebaseline</th>\n",
       "      <th>ERCs_thicknessbaseline</th>\n",
       "      <th>ERCsVolumebaseline</th>\n",
       "      <th>HipposcampusVolumebaseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>81.3479</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158.27</td>\n",
       "      <td>0.63</td>\n",
       "      <td>218.30</td>\n",
       "      <td>28.37</td>\n",
       "      <td>...</td>\n",
       "      <td>253.10</td>\n",
       "      <td>0.40</td>\n",
       "      <td>208.65</td>\n",
       "      <td>23.39</td>\n",
       "      <td>581.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2568.19</td>\n",
       "      <td>2.31</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>3047.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>67.6904</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>147.64</td>\n",
       "      <td>0.55</td>\n",
       "      <td>173.64</td>\n",
       "      <td>44.72</td>\n",
       "      <td>...</td>\n",
       "      <td>220.88</td>\n",
       "      <td>0.48</td>\n",
       "      <td>215.70</td>\n",
       "      <td>33.74</td>\n",
       "      <td>641.90</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4113.01</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>3449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>73.8027</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>199.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>222.27</td>\n",
       "      <td>41.18</td>\n",
       "      <td>...</td>\n",
       "      <td>220.37</td>\n",
       "      <td>0.54</td>\n",
       "      <td>232.18</td>\n",
       "      <td>29.18</td>\n",
       "      <td>708.36</td>\n",
       "      <td>2.87</td>\n",
       "      <td>-1388.41</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2044.0</td>\n",
       "      <td>3441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>84.5945</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>184.21</td>\n",
       "      <td>0.53</td>\n",
       "      <td>201.55</td>\n",
       "      <td>43.04</td>\n",
       "      <td>...</td>\n",
       "      <td>198.42</td>\n",
       "      <td>0.54</td>\n",
       "      <td>220.48</td>\n",
       "      <td>26.68</td>\n",
       "      <td>683.50</td>\n",
       "      <td>2.77</td>\n",
       "      <td>-2506.55</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>2875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>73.9726</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>233.02</td>\n",
       "      <td>0.48</td>\n",
       "      <td>229.88</td>\n",
       "      <td>39.46</td>\n",
       "      <td>...</td>\n",
       "      <td>196.55</td>\n",
       "      <td>0.53</td>\n",
       "      <td>210.63</td>\n",
       "      <td>26.60</td>\n",
       "      <td>645.95</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-1164.02</td>\n",
       "      <td>2.64</td>\n",
       "      <td>1397.0</td>\n",
       "      <td>2700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RID  Gender  Ageatscreening  Diagnosis  MMSE0m  HipsASMbaseline  \\\n",
       "0    3       0         81.3479          3    20.0              NaN   \n",
       "1    4       0         67.6904          1    27.0             0.06   \n",
       "2    5       0         73.8027          0    29.0             0.10   \n",
       "3    8       1         84.5945          0    28.0             0.08   \n",
       "4   10       1         73.9726          3    24.0             0.11   \n",
       "\n",
       "   HipsContrastbaseline  HipsCorelationbaseline  HipsVariancebaseline  \\\n",
       "0                158.27                    0.63                218.30   \n",
       "1                147.64                    0.55                173.64   \n",
       "2                199.66                    0.55                222.27   \n",
       "3                184.21                    0.53                201.55   \n",
       "4                233.02                    0.48                229.88   \n",
       "\n",
       "   HipsSumAveragebaseline  ...  ERCsContrastbaseline  ERCsCorelationbaseline  \\\n",
       "0                   28.37  ...                253.10                    0.40   \n",
       "1                   44.72  ...                220.88                    0.48   \n",
       "2                   41.18  ...                220.37                    0.54   \n",
       "3                   43.04  ...                198.42                    0.54   \n",
       "4                   39.46  ...                196.55                    0.53   \n",
       "\n",
       "   ERCsVariancebaseline  ERCsSumAveragebaseline  ERCsSumVariancebaseline  \\\n",
       "0                208.65                   23.39                   581.50   \n",
       "1                215.70                   33.74                   641.90   \n",
       "2                232.18                   29.18                   708.36   \n",
       "3                220.48                   26.68                   683.50   \n",
       "4                210.63                   26.60                   645.95   \n",
       "\n",
       "   ERCsEntropybaseline  ERCsClusterShadebaseline  ERCs_thicknessbaseline  \\\n",
       "0                  NaN                  -2568.19                    2.31   \n",
       "1                 3.33                   4113.01                    2.76   \n",
       "2                 2.87                  -1388.41                    3.18   \n",
       "3                 2.77                  -2506.55                    2.68   \n",
       "4                 2.72                  -1164.02                    2.64   \n",
       "\n",
       "   ERCsVolumebaseline  HipposcampusVolumebaseline  \n",
       "0              1176.0                      3047.0  \n",
       "1              1942.0                      3449.0  \n",
       "2              2044.0                      3441.0  \n",
       "3              1959.0                      2875.0  \n",
       "4              1397.0                      2700.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv(\"C:/Users/steve/Desktop/Notebooks/Thesis-Project/ADNI(Rawdata).csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((425, 22), (183, 22))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's separate into training and testing set\n",
    "dataset.drop(labels = \"RID\", axis = 1, inplace = True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset.drop(\"Diagnosis\", axis=1),  \n",
    "    dataset[\"Diagnosis\"],  \n",
    "    test_size=0.3,  \n",
    "    random_state=0,  \n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Data imputation & feature scaling\n",
    "\n",
    "Lets extract all columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MMSE0m', 'HipsASMbaseline', 'HipsContrastbaseline',\n",
       "       'HipsCorelationbaseline', 'HipsVariancebaseline',\n",
       "       'HipsSumAveragebaseline', 'HipsSumVariancebaseline',\n",
       "       'HipsEntropybaseline', 'HipsClusterShadebaseline', 'ERCsASMbaseline',\n",
       "       'ERCsContrastbaseline', 'ERCsCorelationbaseline',\n",
       "       'ERCsVariancebaseline', 'ERCsSumAveragebaseline',\n",
       "       'ERCsSumVariancebaseline', 'ERCsEntropybaseline',\n",
       "       'ERCsClusterShadebaseline', 'ERCs_thicknessbaseline',\n",
       "       'ERCsVolumebaseline', 'HipposcampusVolumebaseline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_columns = dataset.columns[dataset.isnull().sum() > 0]\n",
    "na_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"imputer\", MeanMedianImputer(\n",
    "        imputation_method=\"mean\", \n",
    "        variables=[\n",
    "            'MMSE0m', 'HipsASMbaseline', 'HipsContrastbaseline',\n",
    "            'HipsCorelationbaseline', 'HipsVariancebaseline',\n",
    "            'HipsSumAveragebaseline', 'HipsSumVariancebaseline',\n",
    "            'HipsEntropybaseline', 'HipsClusterShadebaseline', \n",
    "            'ERCsASMbaseline', 'ERCsContrastbaseline', \n",
    "            'ERCsCorelationbaseline', 'ERCsVariancebaseline', \n",
    "            'ERCsSumAveragebaseline', 'ERCsSumVariancebaseline',\n",
    "            'ERCsEntropybaseline', 'ERCsClusterShadebaseline', \n",
    "            'ERCs_thicknessbaseline', 'ERCsVolumebaseline', \n",
    "            'HipposcampusVolumebaseline'\n",
    "        ]\n",
    "    )),\n",
    "    (\"scaler\", StandardScaler().set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# let's transform the data with the pipeline\n",
    "X_train_scaled = pipe.transform(X_train)\n",
    "X_test_scaled = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "* Typically, after imputing the dataset, we analyze and visualize the data to assess whether it has been significantly affected. However, for the sake of simplicity, we will temporarily skip this step.\n",
    "* A split analysis will be conducted after the first iteration.\n",
    "* Additionally, no feature selection will be performed until we complete the first iteration.\n",
    "* Variables have not been dropped.\n",
    "* Hyperparameter optimization has not been performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We will experiment with various models that were previously mentioned in paper. \n",
    "\n",
    "* Logistic regression\n",
    "* Support vector machine\n",
    "* Decision tree\n",
    "* Random forest\n",
    "\n",
    "I will only focus on these 4 models for now. Though i would love to check how a simple ANN would work here. I'll try that afterwards.\n",
    "\n",
    "Documentation on sklearn for any of the below models\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "lg = LogisticRegression(multi_class = \"multinomial\", \n",
    "                        solver = \"lbfgs\",\n",
    "                        max_iter = 1000,\n",
    "                        random_state = 42)\n",
    "\n",
    "svm = SVC(kernel ='rbf', \n",
    "          decision_function_shape ='ovo',\n",
    "          probability = True,\n",
    "          random_state = 42)\n",
    "\n",
    "dt = decision_tree_model = DecisionTreeClassifier(\n",
    "     criterion ='gini',      \n",
    "     max_depth = 5,           \n",
    "     min_samples_split = 10,  \n",
    "     min_samples_leaf = 5,    \n",
    "     max_features = 'sqrt',    \n",
    "     random_state = 42)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "     n_estimators = 100,     \n",
    "     criterion = 'gini',     \n",
    "     max_depth = 5,           \n",
    "     min_samples_split = 10, \n",
    "     min_samples_leaf = 5,   \n",
    "     max_features = 'sqrt',   \n",
    "     bootstrap = True,        \n",
    "     random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, it was stated that they performed 5 KFolds, so we will replicate their approach. \n",
    "\n",
    "ROC AUC along with other mentioned metrics will be covered here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "kf = KFold(n_splits = 5, \n",
    "           shuffle = True, \n",
    "           random_state = 42)\n",
    "\n",
    "# Define metrics to evaluate\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average = 'weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average = 'weighted', zero_division=0),\n",
    "    'f1': make_scorer(f1_score, average = 'weighted', zero_division=0),\n",
    "    'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', response_method = \"predict_proba\")\n",
    "}\n",
    "\n",
    "lg_results = cross_validate(\n",
    "    lg,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "svm_results = cross_validate(\n",
    "    svm,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "dt_results = cross_validate(\n",
    "    dt,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)\n",
    "\n",
    "rf_results = cross_validate(\n",
    "    rf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring = scoring_metrics,\n",
    "    return_train_score = True,\n",
    "    cv = kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Logistic Regression\n",
      "Mean train set accuracy: 0.7176470588235294 Â± 0.010685824779167581\n",
      "Mean test set accuracy: 0.628235294117647 Â± 0.021820278812931068\n",
      "Mean train precision: 0.7137715549459933 Â± 0.009994613394817947\n",
      "Mean test precision: 0.6346294513584386 Â± 0.016375364642513202\n",
      "Mean train recall: 0.7176470588235294 Â± 0.010685824779167581\n",
      "Mean test recall: 0.628235294117647 Â± 0.021820278812931068\n",
      "Mean train F1 score: 0.7141321948311611 Â± 0.01008676011818872\n",
      "Mean test F1 score: 0.6264786751157274 Â± 0.018573953162226077\n",
      "Mean train ROC AUC: 0.9092337235320909 Â± 0.005358305500420103\n",
      "Mean test ROC AUC: 0.8598573511351006 Â± 0.021712265716621503 \n",
      "\n",
      "------------------------------------------------------\n",
      "Support Vector Machine\n",
      "Mean train set accuracy: 0.8023529411764706 Â± 0.005060191333554468\n",
      "Mean test set accuracy: 0.5999999999999999 Â± 0.034899757586332535\n",
      "Mean train precision: 0.8076762897810934 Â± 0.004279107619041377\n",
      "Mean test precision: 0.5951572225624752 Â± 0.06058994372992349\n",
      "Mean train recall: 0.8023529411764706 Â± 0.005060191333554468\n",
      "Mean test recall: 0.5999999999999999 Â± 0.034899757586332535\n",
      "Mean train F1 score: 0.7902525236346591 Â± 0.010861552429417795\n",
      "Mean test F1 score: 0.5822790396514332 Â± 0.03686349174882935\n",
      "Mean train ROC AUC: 0.961066882246229 Â± 0.002089761767459735\n",
      "Mean test ROC AUC: 0.8334618581934207 Â± 0.01947680760974415 \n",
      "\n",
      "------------------------------------------------------\n",
      "Decision Tree\n",
      "Mean train set accuracy: 0.6611764705882353 Â± 0.015607646072260716\n",
      "Mean test set accuracy: 0.48470588235294115 Â± 0.03965246952082993\n",
      "Mean train precision: 0.6796646938094781 Â± 0.02293938320519404\n",
      "Mean test precision: 0.5226249600578503 Â± 0.0779000047738367\n",
      "Mean train recall: 0.6611764705882353 Â± 0.015607646072260716\n",
      "Mean test recall: 0.48470588235294115 Â± 0.03965246952082993\n",
      "Mean train F1 score: 0.6570135443289133 Â± 0.02069161904344594\n",
      "Mean test F1 score: 0.4885023003135574 Â± 0.04736948595444704\n",
      "Mean train ROC AUC: 0.8739298010448676 Â± 0.011918136463410983\n",
      "Mean test ROC AUC: 0.6833152973377563 Â± 0.04786030153999176 \n",
      "\n",
      "------------------------------------------------------\n",
      "Random Forest\n",
      "Mean train set accuracy: 0.8288235294117646 Â± 0.007979211744853274\n",
      "Mean test set accuracy: 0.6329411764705882 Â± 0.02919923210821376\n",
      "Mean train precision: 0.8449432677220934 Â± 0.005132214027408043\n",
      "Mean test precision: 0.6291677537030343 Â± 0.04999119327336689\n",
      "Mean train recall: 0.8288235294117646 Â± 0.007979211744853274\n",
      "Mean test recall: 0.6329411764705882 Â± 0.02919923210821376\n",
      "Mean train F1 score: 0.8178129408867527 Â± 0.012721745820971941\n",
      "Mean test F1 score: 0.6014549066181137 Â± 0.0322475963166305\n",
      "Mean train ROC AUC: 0.9784966965143799 Â± 0.0019831194312036346\n",
      "Mean test ROC AUC: 0.8467384070128336 Â± 0.011805596191024063 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results for Logistic Regression\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Logistic Regression\")\n",
    "print('Mean train set accuracy:', np.mean(lg_results['train_accuracy']), 'Â±', np.std(lg_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(lg_results['test_accuracy']), 'Â±', np.std(lg_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(lg_results['train_precision']), 'Â±', np.std(lg_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(lg_results['test_precision']), 'Â±', np.std(lg_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(lg_results['train_recall']), 'Â±', np.std(lg_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(lg_results['test_recall']), 'Â±', np.std(lg_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(lg_results['train_f1']), 'Â±', np.std(lg_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(lg_results['test_f1']), 'Â±', np.std(lg_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(lg_results['train_roc_auc']), 'Â±', np.std(lg_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(lg_results['test_roc_auc']), 'Â±', np.std(lg_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "# Print results for Support Vector Machine\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Support Vector Machine\")\n",
    "print('Mean train set accuracy:', np.mean(svm_results['train_accuracy']), 'Â±', np.std(svm_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(svm_results['test_accuracy']), 'Â±', np.std(svm_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(svm_results['train_precision']), 'Â±', np.std(svm_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(svm_results['test_precision']), 'Â±', np.std(svm_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(svm_results['train_recall']), 'Â±', np.std(svm_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(svm_results['test_recall']), 'Â±', np.std(svm_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(svm_results['train_f1']), 'Â±', np.std(svm_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(svm_results['test_f1']), 'Â±', np.std(svm_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(svm_results['train_roc_auc']), 'Â±', np.std(svm_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(svm_results['test_roc_auc']), 'Â±', np.std(svm_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "# Print results for Decision Tree\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Decision Tree\")\n",
    "print('Mean train set accuracy:', np.mean(dt_results['train_accuracy']), 'Â±', np.std(dt_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(dt_results['test_accuracy']), 'Â±', np.std(dt_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(dt_results['train_precision']), 'Â±', np.std(dt_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(dt_results['test_precision']), 'Â±', np.std(dt_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(dt_results['train_recall']), 'Â±', np.std(dt_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(dt_results['test_recall']), 'Â±', np.std(dt_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(dt_results['train_f1']), 'Â±', np.std(dt_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(dt_results['test_f1']), 'Â±', np.std(dt_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(dt_results['train_roc_auc']), 'Â±', np.std(dt_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(dt_results['test_roc_auc']), 'Â±', np.std(dt_results['test_roc_auc']), \"\\n\")\n",
    "\n",
    "# Print results for Random Forest\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Random Forest\")\n",
    "print('Mean train set accuracy:', np.mean(rf_results['train_accuracy']), 'Â±', np.std(rf_results['train_accuracy']))\n",
    "print('Mean test set accuracy:', np.mean(rf_results['test_accuracy']), 'Â±', np.std(rf_results['test_accuracy']))\n",
    "print('Mean train precision:', np.mean(rf_results['train_precision']), 'Â±', np.std(rf_results['train_precision']))\n",
    "print('Mean test precision:', np.mean(rf_results['test_precision']), 'Â±', np.std(rf_results['test_precision']))\n",
    "print('Mean train recall:', np.mean(rf_results['train_recall']), 'Â±', np.std(rf_results['train_recall']))\n",
    "print('Mean test recall:', np.mean(rf_results['test_recall']), 'Â±', np.std(rf_results['test_recall']))\n",
    "print('Mean train F1 score:', np.mean(rf_results['train_f1']), 'Â±', np.std(rf_results['train_f1']))\n",
    "print('Mean test F1 score:', np.mean(rf_results['test_f1']), 'Â±', np.std(rf_results['test_f1']))\n",
    "print('Mean train ROC AUC:', np.mean(rf_results['train_roc_auc']), 'Â±', np.std(rf_results['train_roc_auc']))\n",
    "print('Mean test ROC AUC:', np.mean(rf_results['test_roc_auc']), 'Â±', np.std(rf_results['test_roc_auc']), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that based on the accuracy metric only, Random Forest can be considered as the best model with a mean test accuracy of 0.6329.\n",
    "While the worst Model would be Decision Tree, with a mean test accuracy of 0.4847."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
